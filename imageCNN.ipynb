{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data', train=True, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 1,28x28\n",
    "        self.conv1=nn.Conv2d(1,10,5) # 10, 24x24\n",
    "        self.conv2=nn.Conv2d(10,20,3) # 128, 10x10\n",
    "        self.fc1 = nn.Linear(20*10*10,500)\n",
    "        self.fc2 = nn.Linear(500,10)\n",
    "    def forward(self,x):\n",
    "        in_size = x.size(0)\n",
    "        out = self.conv1(x) #24\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out, 2, 2)  #12\n",
    "        out = self.conv2(out) #10\n",
    "        out = F.relu(out)\n",
    "        out = out.view(in_size,-1)\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = F.log_softmax(out,dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=2000, out_features=500, bias=True)\n",
      "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[[[-1.5692e-01,  9.1475e-02,  5.2455e-02,  1.7846e-01,  2.1577e-01],\n",
      "          [ 2.3178e-01,  3.1371e-01,  8.5955e-02, -1.8129e-01, -1.1215e-01],\n",
      "          [ 1.5970e-02, -1.1045e-01, -9.1522e-02,  6.2954e-02, -7.4004e-02],\n",
      "          [-1.1620e-01, -1.1466e-01, -2.2851e-01,  1.7146e-01,  2.7885e-02],\n",
      "          [-5.6950e-02, -4.8372e-02,  1.0013e-02,  5.5083e-05, -1.3445e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 9.2209e-02, -1.8548e-01,  8.7760e-02, -8.0922e-02,  3.9589e-02],\n",
      "          [ 1.3693e-01, -1.6525e-01, -3.0846e-01,  1.7289e-02,  1.3112e-01],\n",
      "          [ 2.3782e-01, -1.5258e-01, -1.2569e-01, -2.1420e-01, -1.3906e-01],\n",
      "          [-1.3411e-01,  2.4165e-01,  1.4294e-01,  6.6360e-02, -1.4799e-01],\n",
      "          [ 7.1282e-02,  2.4951e-01,  3.7682e-02,  1.9280e-01,  9.0307e-02]]],\n",
      "\n",
      "\n",
      "        [[[-7.8362e-02,  1.6348e-01,  7.5093e-02,  1.2273e-02,  1.3231e-01],\n",
      "          [-1.4507e-01,  2.2347e-01,  1.5418e-01,  1.4340e-01,  2.1347e-01],\n",
      "          [ 2.3604e-01,  2.1383e-01, -2.2437e-01, -8.2357e-02,  1.3407e-01],\n",
      "          [-1.1316e-01, -1.5272e-01,  1.4372e-01,  1.0952e-01,  1.4795e-01],\n",
      "          [ 1.3383e-01,  9.3209e-02,  2.6218e-01, -7.6698e-02,  1.7375e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.5805e-01, -6.8925e-02,  2.0333e-01,  7.0974e-03, -5.5524e-02],\n",
      "          [ 8.0610e-03,  2.6931e-02,  1.2273e-01, -9.7954e-03, -2.7235e-01],\n",
      "          [ 2.8680e-02,  2.0015e-01, -5.9784e-02, -2.4753e-01,  5.5233e-02],\n",
      "          [-4.8388e-06,  8.9244e-02,  2.3734e-01, -9.3280e-02, -1.1715e-01],\n",
      "          [-1.5118e-01,  2.3298e-01,  9.5986e-02, -1.5794e-01,  1.9947e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.5976e-02, -4.7677e-02, -1.2040e-01, -2.0138e-01, -6.6771e-02],\n",
      "          [ 1.5701e-01,  9.8902e-02, -6.7758e-03,  2.7968e-02, -2.5202e-02],\n",
      "          [-3.5385e-02,  1.1701e-01, -3.9431e-02, -3.2941e-01, -1.2470e-03],\n",
      "          [ 2.5600e-01,  6.7298e-02, -1.7696e-01, -2.3906e-02, -1.4594e-01],\n",
      "          [ 2.5563e-01,  2.1439e-01,  2.1077e-01, -1.5714e-01, -2.0618e-01]]],\n",
      "\n",
      "\n",
      "        [[[-9.6723e-02, -3.0264e-01, -3.4605e-01, -1.0137e-01, -3.2198e-01],\n",
      "          [ 1.7028e-01, -7.5518e-02,  1.3303e-01, -1.9056e-01,  1.3784e-01],\n",
      "          [ 2.6875e-01,  1.8218e-01,  4.4338e-02,  2.1180e-01, -6.4303e-02],\n",
      "          [ 5.6312e-02,  2.7019e-03,  1.4240e-01,  1.7323e-01,  1.7620e-01],\n",
      "          [ 1.2264e-01, -5.0921e-02,  1.1894e-01, -1.9430e-01, -3.8066e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.2368e-01, -2.9623e-01, -2.4268e-01, -2.2499e-01, -2.0263e-01],\n",
      "          [-8.5277e-02, -1.0613e-01,  5.2042e-02,  2.6219e-02,  2.6761e-01],\n",
      "          [-2.1048e-03,  2.3905e-01,  4.4830e-02,  1.8358e-01,  1.1696e-02],\n",
      "          [ 1.1634e-01,  2.5361e-01, -8.2133e-05,  2.3002e-01,  6.7633e-02],\n",
      "          [ 8.1599e-02,  2.3563e-01,  1.5826e-01,  3.2406e-02, -1.3922e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.5852e-02,  2.3861e-01,  9.7320e-03,  1.5651e-01, -4.5904e-02],\n",
      "          [-1.0682e-01, -8.3741e-02,  2.8654e-01,  2.6840e-01,  1.5573e-01],\n",
      "          [-2.6690e-01, -1.8767e-01, -7.1964e-02,  1.4462e-01,  1.4727e-01],\n",
      "          [-9.4865e-02, -2.9686e-01, -1.1098e-01,  1.6993e-01,  2.6595e-02],\n",
      "          [-2.9132e-02, -2.4614e-01, -1.1920e-01, -4.5481e-02,  1.8006e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.5846e-01,  1.8064e-01,  5.8821e-02, -1.4889e-01,  4.8997e-02],\n",
      "          [ 9.7603e-02,  2.2534e-02, -1.1812e-01,  3.9652e-02, -1.9805e-01],\n",
      "          [ 3.9353e-02, -2.5936e-02,  1.6649e-01, -2.8931e-01, -1.2558e-01],\n",
      "          [ 6.6218e-02,  1.0351e-01,  1.5274e-01, -1.7822e-01, -2.8911e-01],\n",
      "          [-6.4149e-02,  3.1137e-01,  2.1338e-01,  2.2211e-01,  9.1145e-02]]],\n",
      "\n",
      "\n",
      "        [[[-9.1269e-03,  5.7160e-02,  2.0004e-01, -2.0331e-01, -5.6829e-02],\n",
      "          [ 1.3611e-01, -1.0462e-04,  9.1039e-02, -1.6028e-01, -8.7966e-02],\n",
      "          [ 8.3989e-02,  2.7346e-01, -2.0490e-01, -3.1280e-01,  7.2019e-02],\n",
      "          [ 2.2367e-01,  2.4187e-02, -1.9198e-01, -2.3169e-01,  1.5925e-01],\n",
      "          [ 2.8402e-01, -4.1579e-02, -3.2292e-01, -2.5896e-01,  2.1271e-01]]]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0066,  0.1619, -0.0621,  0.1375, -0.0463,  0.0862,  0.2133, -0.0320,\n",
      "        -0.0247,  0.0738], requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.0125,  0.0056,  0.0772],\n",
      "          [ 0.1553,  0.0466,  0.1228],\n",
      "          [-0.0149, -0.0276,  0.0170]],\n",
      "\n",
      "         [[ 0.0044,  0.0523, -0.0319],\n",
      "          [ 0.1204, -0.0802, -0.0345],\n",
      "          [ 0.0680, -0.0172,  0.1164]],\n",
      "\n",
      "         [[ 0.0527, -0.0287, -0.1231],\n",
      "          [ 0.0070, -0.0186,  0.1197],\n",
      "          [ 0.0124,  0.0070, -0.0182]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1008,  0.0450,  0.0352],\n",
      "          [ 0.0175, -0.0072, -0.0068],\n",
      "          [ 0.0055,  0.0023, -0.1017]],\n",
      "\n",
      "         [[-0.0169, -0.0928, -0.0838],\n",
      "          [-0.1210, -0.0467,  0.0733],\n",
      "          [ 0.0027, -0.0672,  0.0849]],\n",
      "\n",
      "         [[ 0.0641, -0.0504, -0.0148],\n",
      "          [-0.0975, -0.0666,  0.0799],\n",
      "          [-0.0552, -0.0559, -0.0218]]],\n",
      "\n",
      "\n",
      "        [[[-0.0248,  0.0139,  0.0838],\n",
      "          [ 0.1602, -0.0494,  0.1024],\n",
      "          [ 0.1292,  0.1254,  0.1150]],\n",
      "\n",
      "         [[ 0.1459,  0.0950,  0.0614],\n",
      "          [ 0.0055,  0.1271,  0.1588],\n",
      "          [ 0.0832, -0.1492, -0.1441]],\n",
      "\n",
      "         [[ 0.0868,  0.0310, -0.0758],\n",
      "          [-0.0653, -0.0335, -0.1103],\n",
      "          [ 0.0776, -0.0870,  0.0964]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0398,  0.0230, -0.1560],\n",
      "          [ 0.1436,  0.0858, -0.0157],\n",
      "          [ 0.0842,  0.1573,  0.1218]],\n",
      "\n",
      "         [[ 0.1945,  0.1009,  0.1661],\n",
      "          [-0.0871,  0.0708, -0.0262],\n",
      "          [-0.0306, -0.2070, -0.1074]],\n",
      "\n",
      "         [[-0.0703, -0.1011, -0.0304],\n",
      "          [-0.0872, -0.2855, -0.2315],\n",
      "          [ 0.0233, -0.0400, -0.1813]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0834,  0.0455, -0.0126],\n",
      "          [-0.1488,  0.0411,  0.0606],\n",
      "          [-0.1876, -0.1748, -0.1562]],\n",
      "\n",
      "         [[-0.1240, -0.0273, -0.0961],\n",
      "          [-0.1508, -0.0738,  0.0242],\n",
      "          [-0.1389, -0.0240, -0.0446]],\n",
      "\n",
      "         [[-0.0565,  0.0158,  0.0886],\n",
      "          [ 0.0333, -0.1069, -0.1378],\n",
      "          [-0.0578, -0.0258, -0.0785]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0048, -0.0201,  0.1135],\n",
      "          [ 0.1441, -0.0637, -0.0331],\n",
      "          [ 0.1584,  0.1090,  0.0825]],\n",
      "\n",
      "         [[ 0.0114,  0.0987,  0.0381],\n",
      "          [-0.0301,  0.0693, -0.0578],\n",
      "          [ 0.0320, -0.0139,  0.0994]],\n",
      "\n",
      "         [[ 0.1280,  0.0654,  0.0079],\n",
      "          [ 0.1286, -0.0263,  0.0143],\n",
      "          [ 0.0985,  0.1189,  0.1394]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0264, -0.0417, -0.1320],\n",
      "          [ 0.0529, -0.0131,  0.0712],\n",
      "          [-0.0455,  0.1390,  0.0739]],\n",
      "\n",
      "         [[ 0.0613, -0.0253, -0.0370],\n",
      "          [-0.0974, -0.0279, -0.1103],\n",
      "          [-0.0548, -0.0394, -0.0457]],\n",
      "\n",
      "         [[ 0.0078,  0.0775,  0.0185],\n",
      "          [ 0.0188,  0.0933, -0.0549],\n",
      "          [ 0.0594, -0.0899, -0.0193]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0896, -0.1272, -0.0202],\n",
      "          [-0.0546,  0.0731,  0.0492],\n",
      "          [-0.0155,  0.0529, -0.0503]],\n",
      "\n",
      "         [[ 0.0680, -0.0323, -0.0796],\n",
      "          [-0.1070, -0.1347,  0.0203],\n",
      "          [ 0.0499, -0.0632, -0.0734]],\n",
      "\n",
      "         [[-0.0715, -0.0410, -0.0271],\n",
      "          [-0.0691, -0.0739,  0.0369],\n",
      "          [ 0.0690,  0.0312,  0.0679]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0371, -0.0349,  0.0859],\n",
      "          [ 0.1302,  0.1696,  0.1811],\n",
      "          [ 0.0095,  0.1618,  0.0596]],\n",
      "\n",
      "         [[-0.0816, -0.0333,  0.0380],\n",
      "          [-0.0794,  0.1276,  0.0406],\n",
      "          [-0.0481,  0.0132, -0.0172]],\n",
      "\n",
      "         [[-0.0093, -0.0977,  0.0578],\n",
      "          [-0.0746, -0.0651, -0.0639],\n",
      "          [-0.0547, -0.0794, -0.0049]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0194, -0.0611,  0.1148],\n",
      "          [-0.0270, -0.0492, -0.1148],\n",
      "          [ 0.0270,  0.0507,  0.0167]],\n",
      "\n",
      "         [[ 0.0415,  0.0278,  0.0122],\n",
      "          [-0.1595,  0.1155,  0.0345],\n",
      "          [-0.1278, -0.1650, -0.1469]],\n",
      "\n",
      "         [[ 0.0943, -0.1735,  0.0102],\n",
      "          [ 0.0586, -0.1490, -0.1304],\n",
      "          [ 0.0206,  0.0640, -0.0442]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0316,  0.0920,  0.0584],\n",
      "          [-0.0847,  0.0694,  0.0458],\n",
      "          [ 0.0746,  0.1560,  0.1312]],\n",
      "\n",
      "         [[-0.0933, -0.0617,  0.1333],\n",
      "          [-0.0818,  0.0128,  0.0104],\n",
      "          [ 0.1127, -0.0691,  0.0988]],\n",
      "\n",
      "         [[ 0.0373,  0.0109, -0.0819],\n",
      "          [ 0.0610, -0.0568,  0.0065],\n",
      "          [ 0.0678, -0.0502,  0.0727]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0776,  0.0476, -0.0225],\n",
      "          [ 0.0302,  0.0885,  0.1018],\n",
      "          [-0.1225, -0.0209,  0.0960]],\n",
      "\n",
      "         [[-0.0369,  0.1076, -0.0067],\n",
      "          [-0.0576, -0.0598,  0.0664],\n",
      "          [-0.0465, -0.0645,  0.0684]],\n",
      "\n",
      "         [[-0.1269,  0.1080, -0.0675],\n",
      "          [-0.1293, -0.1145, -0.0558],\n",
      "          [-0.1063,  0.0744, -0.1438]]]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0131, -0.0515,  0.0497, -0.0079, -0.1060,  0.1223, -0.0305, -0.1030,\n",
      "        -0.0711, -0.0404, -0.0584, -0.0305, -0.0732,  0.0360, -0.0308, -0.0640,\n",
      "        -0.0960,  0.1109,  0.0233,  0.0291], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0129,  0.0101,  0.0102,  ...,  0.0172, -0.0048, -0.0441],\n",
      "        [ 0.0493, -0.0424,  0.0107,  ...,  0.0242,  0.0518,  0.0127],\n",
      "        [-0.0035, -0.0096,  0.0055,  ..., -0.0404, -0.0988, -0.0424],\n",
      "        ...,\n",
      "        [-0.0033, -0.0320, -0.0387,  ..., -0.0536,  0.0069,  0.0168],\n",
      "        [ 0.0011, -0.0199,  0.0061,  ...,  0.0134,  0.0134,  0.0229],\n",
      "        [-0.0223,  0.0133,  0.0097,  ...,  0.0157, -0.0282, -0.0106]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 1.8307e-02,  3.4015e-02, -5.1002e-03, -1.1062e-03, -1.2532e-02,\n",
      "         2.4076e-03, -3.1598e-03,  1.0433e-02,  3.4938e-02,  9.8040e-03,\n",
      "         3.4258e-02, -1.2309e-02, -1.5352e-02, -4.2831e-03, -3.5534e-02,\n",
      "         2.6881e-02,  9.7408e-05, -4.1787e-03, -2.2386e-02, -2.0383e-04,\n",
      "        -1.8161e-02, -1.3536e-02,  2.2738e-03, -5.4006e-03,  3.9436e-02,\n",
      "         1.8925e-03, -1.9708e-02, -1.4629e-02,  2.5579e-02,  7.9557e-03,\n",
      "        -1.1808e-02,  2.8966e-02, -3.2249e-03, -5.9357e-04,  3.3816e-02,\n",
      "         7.5775e-03, -4.9542e-03,  1.4386e-02,  9.5147e-03,  2.6156e-02,\n",
      "         2.4427e-02, -8.5334e-03,  1.8225e-02,  3.8820e-03,  1.5884e-02,\n",
      "        -1.2988e-02,  3.4661e-03,  4.4401e-03,  2.7137e-02, -1.5086e-02,\n",
      "        -7.3875e-03, -8.8184e-03,  9.8795e-03, -2.2141e-02,  7.7678e-03,\n",
      "         1.7956e-02, -2.5101e-03,  2.9474e-02,  1.3190e-02, -1.9267e-02,\n",
      "         3.4496e-03, -1.2976e-02, -1.7490e-02,  7.8517e-03,  2.1549e-02,\n",
      "        -5.9444e-03, -9.9676e-04, -2.3937e-02, -1.7922e-02,  2.2682e-02,\n",
      "        -1.9894e-02, -2.3317e-02, -9.5795e-03,  2.9143e-02, -1.8643e-02,\n",
      "         2.2922e-02,  7.0008e-03,  1.5754e-02,  1.5246e-02, -1.1836e-02,\n",
      "         1.5359e-02,  8.5531e-03,  8.8482e-03,  1.3574e-02, -4.4876e-02,\n",
      "        -1.3991e-02,  1.7352e-02,  2.1294e-02,  2.9033e-02,  2.2674e-02,\n",
      "        -1.6664e-02,  1.0203e-02,  2.2491e-02,  3.9477e-02,  4.7757e-03,\n",
      "         1.3419e-03,  2.0622e-02, -8.0569e-03,  2.7259e-02,  1.8221e-02,\n",
      "         1.0144e-02, -1.3803e-02,  1.9428e-02,  2.0024e-02,  1.1102e-04,\n",
      "        -3.9270e-03,  2.0496e-02,  1.2318e-02,  9.9550e-03,  2.4032e-02,\n",
      "         2.5591e-02, -8.2904e-03, -9.8469e-03, -8.5962e-03, -6.2032e-03,\n",
      "        -4.5368e-03, -1.0321e-02, -2.3885e-02, -1.8080e-02,  2.5177e-02,\n",
      "         4.2738e-03,  1.2725e-02, -2.9591e-03, -9.7587e-03, -6.1786e-03,\n",
      "        -2.7276e-02,  8.5921e-03,  2.6160e-02,  1.0895e-03, -9.2041e-03,\n",
      "         2.0750e-02, -1.0738e-03,  2.2542e-02, -7.0923e-03, -3.3398e-04,\n",
      "         2.8886e-02,  1.0262e-02, -1.7286e-03, -2.9801e-03,  5.3594e-03,\n",
      "        -3.4893e-03, -9.9136e-03,  1.6096e-02, -9.7497e-03,  1.6145e-03,\n",
      "        -1.5744e-02,  1.7326e-02,  5.6531e-03,  2.4022e-02,  1.1675e-02,\n",
      "        -2.6819e-03, -9.3015e-04, -1.7282e-05, -3.2436e-02,  1.9467e-03,\n",
      "        -1.1500e-02, -7.1813e-05, -4.2113e-03,  1.9027e-02,  2.6252e-03,\n",
      "         2.0394e-02,  3.4596e-02,  3.1024e-02,  1.0621e-02, -8.5296e-03,\n",
      "         1.3596e-03, -3.5587e-03,  2.9543e-02, -2.3764e-05, -1.2921e-02,\n",
      "        -5.4915e-03,  2.3294e-02,  1.8439e-02,  1.8533e-02, -1.9454e-02,\n",
      "         1.0383e-02,  2.8625e-02,  4.0993e-03,  1.6289e-02, -1.0901e-02,\n",
      "        -1.9340e-02,  2.8585e-02, -7.5156e-03, -4.7934e-03,  1.4798e-02,\n",
      "         2.0603e-02, -2.2850e-02,  1.7008e-02,  1.8674e-02, -4.0621e-02,\n",
      "        -2.1804e-02,  1.5259e-03, -9.2695e-03,  2.8883e-02,  6.4872e-03,\n",
      "         2.5002e-02,  2.4229e-02,  2.3067e-02, -1.4961e-02, -9.3606e-04,\n",
      "         1.5126e-03, -2.1816e-03, -1.2556e-02, -1.7320e-02,  1.2556e-02,\n",
      "        -4.6398e-03, -6.4506e-03,  2.0517e-02,  3.6484e-02, -1.4996e-02,\n",
      "         8.7906e-03,  1.5406e-02, -4.2124e-03,  3.7496e-02,  2.7750e-02,\n",
      "         1.3384e-02, -1.7297e-02,  8.6790e-03,  1.6795e-02, -1.4215e-02,\n",
      "        -2.6353e-02, -8.3092e-03,  1.4555e-02,  1.5586e-02,  2.6045e-02,\n",
      "        -1.9346e-02,  2.9512e-02,  3.1961e-02,  1.3102e-02, -4.0800e-03,\n",
      "        -2.1705e-02, -5.1722e-03, -1.7634e-02, -7.1637e-05,  1.8988e-02,\n",
      "         1.1798e-02, -4.9593e-03, -3.3884e-03,  2.1618e-02,  3.3173e-02,\n",
      "         4.0715e-03, -1.4996e-02, -7.4209e-04, -4.3750e-03,  8.2148e-03,\n",
      "        -1.3522e-02,  2.2235e-04,  2.5587e-02,  1.9161e-02,  1.5843e-02,\n",
      "        -9.2041e-03, -2.3200e-02,  1.3217e-02,  8.7971e-03, -1.7319e-02,\n",
      "         2.7310e-02,  2.2145e-02,  5.1256e-03,  7.7424e-04, -7.9527e-03,\n",
      "        -4.8854e-03,  6.5902e-03,  1.5703e-02, -2.9714e-03,  4.6930e-04,\n",
      "         2.4704e-04, -5.8862e-04,  3.8255e-04, -2.2235e-02, -1.3565e-02,\n",
      "        -1.2202e-04,  3.7588e-03, -6.0705e-04, -2.1062e-03,  1.9630e-03,\n",
      "        -1.6968e-02,  2.1439e-02,  1.8096e-02, -1.3591e-02,  2.4640e-03,\n",
      "        -1.5417e-02, -1.0421e-02, -6.5032e-03, -1.2025e-02, -2.1053e-02,\n",
      "        -1.0055e-02, -4.6062e-03,  1.7091e-02, -2.5433e-02,  4.0806e-03,\n",
      "        -2.0290e-02, -9.7563e-03,  3.0638e-03,  1.5273e-02,  6.1393e-03,\n",
      "         5.3398e-03,  2.5894e-03, -1.1931e-02, -1.8156e-02,  1.7487e-02,\n",
      "         1.2705e-02, -1.4772e-02, -1.9731e-02,  2.6285e-02,  6.1313e-03,\n",
      "         5.4124e-03, -3.1173e-03, -1.2650e-02,  2.0308e-02, -2.5527e-02,\n",
      "        -1.0888e-02, -1.6592e-02,  1.0279e-02,  6.2136e-03, -2.7985e-02,\n",
      "        -2.5842e-02,  2.4214e-02,  1.2196e-02,  2.1014e-02, -1.3235e-02,\n",
      "        -3.1839e-03,  9.9608e-03,  2.8084e-02,  3.1343e-02,  1.0492e-02,\n",
      "        -3.0247e-03, -6.7982e-03, -2.3760e-02, -1.3860e-02,  9.1292e-03,\n",
      "        -2.7053e-02, -6.2715e-03,  1.9640e-02,  2.6841e-02,  2.9261e-02,\n",
      "        -8.5939e-03, -1.0358e-02, -5.8249e-03,  1.0529e-02, -1.2894e-02,\n",
      "        -4.2420e-03, -2.7516e-03, -2.1392e-02, -3.0885e-03, -1.6381e-02,\n",
      "        -7.6685e-03,  6.5057e-03,  6.1154e-03,  3.3005e-03,  6.7474e-03,\n",
      "         1.5434e-04,  5.9023e-03, -1.6425e-02, -7.5898e-03,  7.9301e-03,\n",
      "         1.9182e-03,  8.6047e-03,  1.5908e-02,  2.6129e-02, -3.5319e-03,\n",
      "        -3.0288e-03, -4.4777e-03, -2.0740e-02,  9.5091e-03,  2.4710e-03,\n",
      "         2.1654e-02,  1.4034e-02,  1.8850e-02, -9.1148e-03,  2.1248e-03,\n",
      "        -1.6454e-02, -4.6278e-03,  2.1487e-02,  9.4157e-03,  1.7891e-02,\n",
      "         5.9653e-03, -2.0541e-03,  1.2223e-02,  3.8018e-03,  5.0940e-04,\n",
      "         3.4629e-02,  2.2115e-02, -3.7219e-03, -1.3036e-02, -1.7883e-03,\n",
      "         8.9437e-03,  2.2751e-02, -2.2102e-02, -1.5631e-02, -2.5208e-02,\n",
      "        -1.5604e-02, -2.5610e-02, -1.1219e-02,  8.7009e-03, -3.2205e-02,\n",
      "         3.2684e-02, -2.3090e-02,  1.9172e-02, -2.1069e-02,  1.0369e-03,\n",
      "         1.9588e-02,  2.1837e-02, -2.4049e-02,  2.0159e-02,  1.6432e-02,\n",
      "        -2.5902e-03,  3.1197e-03,  5.7387e-03, -1.3607e-02, -9.3900e-03,\n",
      "        -1.4593e-02,  8.4876e-04, -5.6765e-04, -8.2150e-03, -2.7202e-03,\n",
      "         2.6420e-02, -4.0892e-03,  1.0751e-02, -9.7193e-03, -3.6960e-03,\n",
      "         1.6758e-02,  5.0299e-03, -8.3891e-03,  7.9755e-03, -7.3968e-03,\n",
      "        -3.7638e-03,  1.4112e-02, -4.3395e-03, -1.4903e-02,  1.9768e-02,\n",
      "         9.6812e-04, -1.8874e-02,  2.4727e-03,  3.9794e-03,  9.3713e-03,\n",
      "        -9.7023e-03, -1.5426e-02, -2.2925e-02,  1.0888e-02,  2.3284e-02,\n",
      "         1.5028e-02, -2.8804e-04, -3.6882e-04,  9.5740e-03,  2.1041e-02,\n",
      "        -1.5410e-02,  2.5631e-02,  7.5063e-03,  2.6060e-02,  1.8316e-02,\n",
      "        -1.5449e-03,  2.2950e-02, -1.3393e-02,  2.0036e-02, -9.1776e-03,\n",
      "        -2.6276e-03,  8.0441e-03,  6.3772e-03, -9.2905e-03,  1.0061e-02,\n",
      "        -1.2717e-02, -1.0307e-04,  2.4509e-02, -2.4913e-03, -8.4790e-03,\n",
      "         1.0384e-02,  2.1008e-02, -1.8856e-02, -7.9187e-03,  1.0063e-02,\n",
      "         2.7474e-03,  1.3706e-02, -3.8259e-03,  3.3144e-03, -6.4549e-04,\n",
      "        -1.3334e-02,  2.7489e-02,  3.3648e-02,  3.1103e-03,  5.4011e-03,\n",
      "        -1.8788e-02,  1.0250e-02,  2.1582e-02, -2.7473e-03, -1.2966e-02,\n",
      "        -3.0318e-02,  3.4070e-03,  1.9405e-02, -1.3434e-02, -1.2994e-02,\n",
      "        -3.1734e-03, -9.1193e-03,  1.1501e-02, -2.8056e-02,  1.6368e-02,\n",
      "        -6.0178e-03, -1.8616e-03,  2.8244e-03,  1.0647e-02,  5.4050e-04],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0260, -0.0196, -0.0277,  ...,  0.0454,  0.0261, -0.0462],\n",
      "        [-0.0652,  0.0321, -0.1056,  ..., -0.0207, -0.0204,  0.0280],\n",
      "        [ 0.0357,  0.0694, -0.0978,  ..., -0.0958,  0.0032, -0.0358],\n",
      "        ...,\n",
      "        [ 0.0888, -0.0306, -0.0169,  ...,  0.0307, -0.0113, -0.0340],\n",
      "        [-0.0386,  0.0649, -0.0250,  ...,  0.0080, -0.0359,  0.0374],\n",
      "        [ 0.0108, -0.0065,  0.0057,  ...,  0.0517, -0.0335, -0.0458]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0097, -0.0112,  0.0078, -0.0350, -0.0506, -0.0478,  0.0257, -0.0067,\n",
      "        -0.0085,  0.0211], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "params = list(model.parameters())\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if(batch_idx+1)%30 == 0: \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # 将一批的损失相加\n",
    "            pred = output.max(1, keepdim=True)[1] # 找到概率最大的下标\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [14848/60000 (25%)]\tLoss: 0.315953\n",
      "Train Epoch: 1 [30208/60000 (50%)]\tLoss: 0.256822\n",
      "Train Epoch: 1 [45568/60000 (75%)]\tLoss: 0.109840\n",
      "\n",
      "Test set: Average loss: 0.0964, Accuracy: 9698/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [14848/60000 (25%)]\tLoss: 0.103537\n",
      "Train Epoch: 2 [30208/60000 (50%)]\tLoss: 0.080122\n",
      "Train Epoch: 2 [45568/60000 (75%)]\tLoss: 0.080824\n",
      "\n",
      "Test set: Average loss: 0.0573, Accuracy: 9810/10000 (98%)\n",
      "\n",
      "Train Epoch: 3 [14848/60000 (25%)]\tLoss: 0.093928\n",
      "Train Epoch: 3 [30208/60000 (50%)]\tLoss: 0.051579\n",
      "Train Epoch: 3 [45568/60000 (75%)]\tLoss: 0.041413\n",
      "\n",
      "Test set: Average loss: 0.0395, Accuracy: 9867/10000 (99%)\n",
      "\n",
      "Train Epoch: 4 [14848/60000 (25%)]\tLoss: 0.052614\n",
      "Train Epoch: 4 [30208/60000 (50%)]\tLoss: 0.031835\n",
      "Train Epoch: 4 [45568/60000 (75%)]\tLoss: 0.048364\n",
      "\n",
      "Test set: Average loss: 0.0463, Accuracy: 9855/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [14848/60000 (25%)]\tLoss: 0.031126\n",
      "Train Epoch: 5 [30208/60000 (50%)]\tLoss: 0.028782\n",
      "Train Epoch: 5 [45568/60000 (75%)]\tLoss: 0.040739\n",
      "\n",
      "Test set: Average loss: 0.0325, Accuracy: 9890/10000 (99%)\n",
      "\n",
      "Train Epoch: 6 [14848/60000 (25%)]\tLoss: 0.024422\n",
      "Train Epoch: 6 [30208/60000 (50%)]\tLoss: 0.025973\n",
      "Train Epoch: 6 [45568/60000 (75%)]\tLoss: 0.010296\n",
      "\n",
      "Test set: Average loss: 0.0414, Accuracy: 9868/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [14848/60000 (25%)]\tLoss: 0.026457\n",
      "Train Epoch: 7 [30208/60000 (50%)]\tLoss: 0.011947\n",
      "Train Epoch: 7 [45568/60000 (75%)]\tLoss: 0.071601\n",
      "\n",
      "Test set: Average loss: 0.0357, Accuracy: 9884/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [14848/60000 (25%)]\tLoss: 0.012813\n",
      "Train Epoch: 8 [30208/60000 (50%)]\tLoss: 0.017378\n",
      "Train Epoch: 8 [45568/60000 (75%)]\tLoss: 0.015967\n",
      "\n",
      "Test set: Average loss: 0.0330, Accuracy: 9895/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [14848/60000 (25%)]\tLoss: 0.012066\n",
      "Train Epoch: 9 [30208/60000 (50%)]\tLoss: 0.018112\n",
      "Train Epoch: 9 [45568/60000 (75%)]\tLoss: 0.009990\n",
      "\n",
      "Test set: Average loss: 0.0315, Accuracy: 9899/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [14848/60000 (25%)]\tLoss: 0.012777\n",
      "Train Epoch: 10 [30208/60000 (50%)]\tLoss: 0.005297\n",
      "Train Epoch: 10 [45568/60000 (75%)]\tLoss: 0.005798\n",
      "\n",
      "Test set: Average loss: 0.0375, Accuracy: 9886/10000 (99%)\n",
      "\n",
      "Train Epoch: 11 [14848/60000 (25%)]\tLoss: 0.006116\n",
      "Train Epoch: 11 [30208/60000 (50%)]\tLoss: 0.007204\n",
      "Train Epoch: 11 [45568/60000 (75%)]\tLoss: 0.005599\n",
      "\n",
      "Test set: Average loss: 0.0313, Accuracy: 9914/10000 (99%)\n",
      "\n",
      "Train Epoch: 12 [14848/60000 (25%)]\tLoss: 0.003950\n",
      "Train Epoch: 12 [30208/60000 (50%)]\tLoss: 0.007305\n",
      "Train Epoch: 12 [45568/60000 (75%)]\tLoss: 0.005829\n",
      "\n",
      "Test set: Average loss: 0.0340, Accuracy: 9906/10000 (99%)\n",
      "\n",
      "Train Epoch: 13 [14848/60000 (25%)]\tLoss: 0.005032\n",
      "Train Epoch: 13 [30208/60000 (50%)]\tLoss: 0.008791\n",
      "Train Epoch: 13 [45568/60000 (75%)]\tLoss: 0.002899\n",
      "\n",
      "Test set: Average loss: 0.0317, Accuracy: 9908/10000 (99%)\n",
      "\n",
      "Train Epoch: 14 [14848/60000 (25%)]\tLoss: 0.002078\n",
      "Train Epoch: 14 [30208/60000 (50%)]\tLoss: 0.009002\n",
      "Train Epoch: 14 [45568/60000 (75%)]\tLoss: 0.007123\n",
      "\n",
      "Test set: Average loss: 0.0320, Accuracy: 9909/10000 (99%)\n",
      "\n",
      "Train Epoch: 15 [14848/60000 (25%)]\tLoss: 0.001269\n",
      "Train Epoch: 15 [30208/60000 (50%)]\tLoss: 0.001675\n",
      "Train Epoch: 15 [45568/60000 (75%)]\tLoss: 0.001595\n",
      "\n",
      "Test set: Average loss: 0.0346, Accuracy: 9898/10000 (99%)\n",
      "\n",
      "Train Epoch: 16 [14848/60000 (25%)]\tLoss: 0.001391\n",
      "Train Epoch: 16 [30208/60000 (50%)]\tLoss: 0.002259\n",
      "Train Epoch: 16 [45568/60000 (75%)]\tLoss: 0.005963\n",
      "\n",
      "Test set: Average loss: 0.0371, Accuracy: 9901/10000 (99%)\n",
      "\n",
      "Train Epoch: 17 [14848/60000 (25%)]\tLoss: 0.014808\n",
      "Train Epoch: 17 [30208/60000 (50%)]\tLoss: 0.001863\n",
      "Train Epoch: 17 [45568/60000 (75%)]\tLoss: 0.001962\n",
      "\n",
      "Test set: Average loss: 0.0363, Accuracy: 9892/10000 (99%)\n",
      "\n",
      "Train Epoch: 18 [14848/60000 (25%)]\tLoss: 0.005758\n",
      "Train Epoch: 18 [30208/60000 (50%)]\tLoss: 0.005965\n",
      "Train Epoch: 18 [45568/60000 (75%)]\tLoss: 0.001729\n",
      "\n",
      "Test set: Average loss: 0.0386, Accuracy: 9904/10000 (99%)\n",
      "\n",
      "Train Epoch: 19 [14848/60000 (25%)]\tLoss: 0.001327\n",
      "Train Epoch: 19 [30208/60000 (50%)]\tLoss: 0.000975\n",
      "Train Epoch: 19 [45568/60000 (75%)]\tLoss: 0.001727\n",
      "\n",
      "Test set: Average loss: 0.0371, Accuracy: 9905/10000 (99%)\n",
      "\n",
      "Train Epoch: 20 [14848/60000 (25%)]\tLoss: 0.002662\n",
      "Train Epoch: 20 [30208/60000 (50%)]\tLoss: 0.001109\n",
      "Train Epoch: 20 [45568/60000 (75%)]\tLoss: 0.005475\n",
      "\n",
      "Test set: Average loss: 0.0471, Accuracy: 9887/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model,train_loader, optimizer, epoch)\n",
    "    test(model,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch for Deeplearning",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
